{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import pickle\n",
    "import PIL.Image as Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Running on cuda\n"
     ]
    }
   ],
   "source": [
    "dataset = datasets.CIFAR100(\"../Data\", download=True)\n",
    "tt = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.CenterCrop(32),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "tp = transforms.ToPILImage()\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "print(\"Running on %s\" % device)\n",
    "\n",
    "def label_to_onehot(target, num_classes=100):\n",
    "    target = torch.unsqueeze(target, 1)\n",
    "    onehot_target = torch.zeros(target.size(0), num_classes, device=target.device)\n",
    "    onehot_target.scatter_(1, target, 1)\n",
    "    return onehot_target\n",
    "\n",
    "def cross_entropy_for_onehot(pred, target):\n",
    "    return torch.mean(torch.sum(- target * F.log_softmax(pred, dim=-1), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self, channel=3, hideen=768, num_classes=10):\n",
    "        super(LeNet, self).__init__()\n",
    "        act = nn.Sigmoid\n",
    "        self.body = nn.Sequential(\n",
    "            nn.Conv2d(channel, 12, kernel_size=5, padding=5 // 2, stride=2),\n",
    "            act(),\n",
    "            nn.Conv2d(12, 12, kernel_size=5, padding=5 // 2, stride=2),\n",
    "            act(),\n",
    "            nn.Conv2d(12, 12, kernel_size=5, padding=5 // 2, stride=1),\n",
    "            act(),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hideen, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.body(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    try:\n",
    "        if hasattr(m, \"weight\"):\n",
    "            m.weight.data.uniform_(-0.5, 0.5)\n",
    "    except Exception:\n",
    "        print('warning: failed in weights_init for %s.weight' % m._get_name())\n",
    "    try:\n",
    "        if hasattr(m, \"bias\"):\n",
    "            m.bias.data.uniform_(-0.5, 0.5)\n",
    "    except Exception:\n",
    "        print('warning: failed in weights_init for %s.bias' % m._get_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cifar100 root_path: .\n",
      "cifar100 data_path: ./../Data\n",
      "cifar100 save_path: ./results/DLG_cifar100\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "dataset = 'cifar100'\n",
    "root_path = '.'\n",
    "data_path = os.path.join(root_path, '../Data')\n",
    "save_path = os.path.join(root_path, 'results/DLG_%s'%dataset)\n",
    "\n",
    "lr = 1.0\n",
    "num_dummy = 1\n",
    "Iteration = 300\n",
    "num_exp = 10\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = 'cuda' if use_cuda else 'cpu'\n",
    "\n",
    "tt = transforms.Compose([transforms.ToTensor()])\n",
    "tp = transforms.Compose([transforms.ToPILImage()])\n",
    "\n",
    "print(dataset, 'root_path:', root_path)\n",
    "print(dataset, 'data_path:', data_path)\n",
    "print(dataset, 'save_path:', save_path)\n",
    "\n",
    "if not os.path.exists('results'):\n",
    "    os.mkdir('results')\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "\n",
    "\n",
    "\n",
    "''' load data '''\n",
    "\n",
    "if dataset == 'cifar100':\n",
    "    shape_img = (32, 32)\n",
    "    num_classes = 100\n",
    "    channel = 3\n",
    "    hidden = 768\n",
    "    dataset = datasets.CIFAR100(data_path, download=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running 0|10 experiment\n",
      "DLG, Try to generate 1 images\n",
      "lr = 1.0\n",
      "[2025-02-09 06:06:04] 0 loss = 59.36498260, mse = 1.30176103\n",
      "[2025-02-09 06:06:06] 10 loss = 1.09616494, mse = 0.47565717\n",
      "[2025-02-09 06:06:09] 20 loss = 0.08930164, mse = 0.13494834\n",
      "[2025-02-09 06:06:11] 30 loss = 0.01289919, mse = 0.03734213\n",
      "[2025-02-09 06:06:14] 40 loss = 0.00215783, mse = 0.00954075\n",
      "[2025-02-09 06:06:16] 50 loss = 0.00048877, mse = 0.00256131\n",
      "[2025-02-09 06:06:19] 60 loss = 0.00011931, mse = 0.00069342\n",
      "[2025-02-09 06:06:21] 70 loss = 0.00003694, mse = 0.00022138\n",
      "[2025-02-09 06:06:24] 80 loss = 0.00001551, mse = 0.00008086\n",
      "[2025-02-09 06:06:26] 90 loss = 0.00000980, mse = 0.00003897\n",
      "[2025-02-09 06:06:29] 100 loss = 0.00000671, mse = 0.00001751\n",
      "[2025-02-09 06:06:31] 110 loss = 0.00000538, mse = 0.00000969\n",
      "[2025-02-09 06:06:33] 120 loss = 0.00000444, mse = 0.00000492\n",
      "[2025-02-09 06:06:36] 130 loss = 0.00000393, mse = 0.00000299\n",
      "[2025-02-09 06:06:38] 140 loss = 0.00000356, mse = 0.00000246\n",
      "[2025-02-09 06:06:40] 150 loss = 0.00000325, mse = 0.00000225\n",
      "[2025-02-09 06:06:43] 160 loss = 0.00000298, mse = 0.00000221\n",
      "[2025-02-09 06:06:44] 170 loss = 0.00000298, mse = 0.00000218\n",
      "[2025-02-09 06:06:45] 180 loss = 0.00000298, mse = 0.00000218\n",
      "[2025-02-09 06:06:45] 190 loss = 0.00000298, mse = 0.00000218\n",
      "[2025-02-09 06:06:46] 200 loss = 0.00000298, mse = 0.00000218\n",
      "[2025-02-09 06:06:47] 210 loss = 0.00000298, mse = 0.00000218\n",
      "[2025-02-09 06:06:48] 220 loss = 0.00000298, mse = 0.00000218\n",
      "[2025-02-09 06:06:49] 230 loss = 0.00000298, mse = 0.00000218\n",
      "[2025-02-09 06:06:50] 240 loss = 0.00000298, mse = 0.00000218\n",
      "[2025-02-09 06:06:50] 250 loss = 0.00000298, mse = 0.00000218\n",
      "[2025-02-09 06:06:51] 260 loss = 0.00000298, mse = 0.00000218\n",
      "[2025-02-09 06:06:52] 270 loss = 0.00000298, mse = 0.00000218\n",
      "[2025-02-09 06:06:53] 280 loss = 0.00000298, mse = 0.00000218\n",
      "[2025-02-09 06:06:54] 290 loss = 0.00000298, mse = 0.00000218\n",
      "imidx_list: [18434]\n",
      "loss_DLG: 2.9760906272713328e-06\n",
      "mse_DLG: 2.1793573523609666e-06\n",
      "gt_label: [12] lab_DLG: 12\n",
      "----------------------\n",
      "\n",
      "\n",
      "running 1|10 experiment\n",
      "DLG, Try to generate 1 images\n",
      "lr = 1.0\n",
      "[2025-02-09 06:06:56] 0 loss = 163.73606873, mse = 1.59811485\n",
      "[2025-02-09 06:06:56] 10 loss = 461.48083496, mse = 46949868.00000000\n",
      "[2025-02-09 06:06:56] 20 loss = 461.48083496, mse = 46949868.00000000\n",
      "[2025-02-09 06:06:57] 30 loss = 461.48083496, mse = 46949868.00000000\n",
      "[2025-02-09 06:06:57] 40 loss = 461.48083496, mse = 46949868.00000000\n",
      "[2025-02-09 06:06:57] 50 loss = 461.48083496, mse = 46949868.00000000\n",
      "[2025-02-09 06:06:58] 60 loss = 461.48083496, mse = 46949868.00000000\n",
      "[2025-02-09 06:06:58] 70 loss = 461.48083496, mse = 46949868.00000000\n",
      "[2025-02-09 06:06:58] 80 loss = 461.48083496, mse = 46949868.00000000\n",
      "[2025-02-09 06:06:59] 90 loss = 461.48083496, mse = 46949868.00000000\n",
      "[2025-02-09 06:06:59] 100 loss = 461.48083496, mse = 46949868.00000000\n",
      "[2025-02-09 06:07:00] 110 loss = 461.48083496, mse = 46949868.00000000\n",
      "[2025-02-09 06:07:00] 120 loss = 461.48083496, mse = 46949868.00000000\n",
      "[2025-02-09 06:07:00] 130 loss = 461.48083496, mse = 46949868.00000000\n",
      "[2025-02-09 06:07:01] 140 loss = 461.48083496, mse = 46949868.00000000\n",
      "[2025-02-09 06:07:02] 150 loss = 461.48083496, mse = 46949868.00000000\n",
      "[2025-02-09 06:07:02] 160 loss = 461.48083496, mse = 46949868.00000000\n",
      "[2025-02-09 06:07:03] 170 loss = 461.48083496, mse = 46949868.00000000\n",
      "[2025-02-09 06:07:03] 180 loss = 461.48083496, mse = 46949868.00000000\n",
      "[2025-02-09 06:07:04] 190 loss = 461.48083496, mse = 46949868.00000000\n",
      "[2025-02-09 06:07:05] 200 loss = 461.48083496, mse = 46949868.00000000\n",
      "[2025-02-09 06:07:12] 210 loss = 461.48083496, mse = 46949868.00000000\n",
      "[2025-02-09 06:07:13] 220 loss = 461.48083496, mse = 46949868.00000000\n",
      "[2025-02-09 06:07:13] 230 loss = 461.48083496, mse = 46949868.00000000\n",
      "[2025-02-09 06:07:14] 240 loss = 461.48083496, mse = 46949868.00000000\n",
      "[2025-02-09 06:07:15] 250 loss = 461.48083496, mse = 46949868.00000000\n",
      "[2025-02-09 06:07:16] 260 loss = 461.48083496, mse = 46949868.00000000\n",
      "[2025-02-09 06:07:16] 270 loss = 461.48083496, mse = 46949868.00000000\n",
      "[2025-02-09 06:07:17] 280 loss = 461.48083496, mse = 46949868.00000000\n",
      "[2025-02-09 06:07:18] 290 loss = 461.48083496, mse = 46949868.00000000\n",
      "imidx_list: [9794]\n",
      "loss_DLG: 461.4808349609375\n",
      "mse_DLG: 46949868.0\n",
      "gt_label: [45] lab_DLG: 45\n",
      "----------------------\n",
      "\n",
      "\n",
      "running 2|10 experiment\n",
      "DLG, Try to generate 1 images\n",
      "lr = 1.0\n",
      "[2025-02-09 06:07:20] 0 loss = 87.96571350, mse = 1.23193467\n",
      "[2025-02-09 06:07:22] 10 loss = 1.69279969, mse = 0.42655015\n",
      "[2025-02-09 06:07:24] 20 loss = 0.12783574, mse = 0.13127814\n",
      "[2025-02-09 06:07:27] 30 loss = 0.02449210, mse = 0.05226368\n",
      "[2025-02-09 06:07:29] 40 loss = 0.00609053, mse = 0.02029764\n",
      "[2025-02-09 06:07:32] 50 loss = 0.00180132, mse = 0.00789507\n",
      "[2025-02-09 06:07:34] 60 loss = 0.00067550, mse = 0.00351897\n",
      "[2025-02-09 06:07:37] 70 loss = 0.00024847, mse = 0.00144051\n",
      "[2025-02-09 06:07:40] 80 loss = 0.00010312, mse = 0.00063206\n",
      "[2025-02-09 06:07:42] 90 loss = 0.00004710, mse = 0.00028985\n",
      "[2025-02-09 06:07:45] 100 loss = 0.00002447, mse = 0.00014695\n",
      "[2025-02-09 06:07:47] 110 loss = 0.00001553, mse = 0.00008442\n",
      "[2025-02-09 06:07:49] 120 loss = 0.00001156, mse = 0.00005642\n",
      "[2025-02-09 06:07:52] 130 loss = 0.00000850, mse = 0.00003312\n",
      "[2025-02-09 06:07:54] 140 loss = 0.00000698, mse = 0.00002121\n",
      "[2025-02-09 06:07:57] 150 loss = 0.00000601, mse = 0.00001392\n",
      "[2025-02-09 06:07:59] 160 loss = 0.00000516, mse = 0.00000927\n",
      "[2025-02-09 06:08:02] 170 loss = 0.00000463, mse = 0.00000622\n",
      "[2025-02-09 06:08:04] 180 loss = 0.00000424, mse = 0.00000429\n",
      "[2025-02-09 06:08:07] 190 loss = 0.00000386, mse = 0.00000299\n",
      "[2025-02-09 06:08:09] 200 loss = 0.00000363, mse = 0.00000260\n",
      "[2025-02-09 06:08:12] 210 loss = 0.00000353, mse = 0.00000229\n",
      "[2025-02-09 06:08:15] 220 loss = 0.00000346, mse = 0.00000216\n",
      "[2025-02-09 06:08:17] 230 loss = 0.00000328, mse = 0.00000217\n",
      "[2025-02-09 06:08:18] 240 loss = 0.00000328, mse = 0.00000217\n",
      "[2025-02-09 06:08:19] 250 loss = 0.00000328, mse = 0.00000217\n",
      "[2025-02-09 06:08:20] 260 loss = 0.00000328, mse = 0.00000217\n",
      "[2025-02-09 06:08:21] 270 loss = 0.00000328, mse = 0.00000217\n",
      "[2025-02-09 06:08:22] 280 loss = 0.00000328, mse = 0.00000217\n",
      "[2025-02-09 06:08:23] 290 loss = 0.00000328, mse = 0.00000217\n",
      "imidx_list: [22918]\n",
      "loss_DLG: 3.28276496475155e-06\n",
      "mse_DLG: 2.166405010939343e-06\n",
      "gt_label: [83] lab_DLG: 83\n",
      "----------------------\n",
      "\n",
      "\n",
      "running 3|10 experiment\n",
      "DLG, Try to generate 1 images\n",
      "lr = 1.0\n",
      "[2025-02-09 06:08:24] 0 loss = 101.49759674, mse = 1.69457412\n",
      "[2025-02-09 06:08:26] 10 loss = 2.75630140, mse = 0.81285018\n",
      "[2025-02-09 06:08:29] 20 loss = 0.44499400, mse = 0.34670621\n",
      "[2025-02-09 06:08:31] 30 loss = 0.08117878, mse = 0.14167786\n",
      "[2025-02-09 06:08:33] 40 loss = 0.01592798, mse = 0.06387971\n",
      "[2025-02-09 06:08:36] 50 loss = 0.00491126, mse = 0.03308975\n",
      "[2025-02-09 06:08:38] 60 loss = 0.00182957, mse = 0.01672583\n",
      "[2025-02-09 06:08:41] 70 loss = 0.00078604, mse = 0.00944163\n",
      "[2025-02-09 06:08:44] 80 loss = 0.00037231, mse = 0.00563113\n",
      "[2025-02-09 06:08:46] 90 loss = 0.00019867, mse = 0.00346489\n",
      "[2025-02-09 06:08:49] 100 loss = 0.00010440, mse = 0.00196401\n",
      "[2025-02-09 06:08:52] 110 loss = 0.00006111, mse = 0.00123303\n",
      "[2025-02-09 06:08:55] 120 loss = 0.00003835, mse = 0.00078651\n",
      "[2025-02-09 06:08:58] 130 loss = 0.00002375, mse = 0.00047926\n",
      "[2025-02-09 06:09:01] 140 loss = 0.00001530, mse = 0.00029453\n",
      "[2025-02-09 06:09:03] 150 loss = 0.00001051, mse = 0.00019589\n",
      "[2025-02-09 06:09:06] 160 loss = 0.00000753, mse = 0.00013453\n",
      "[2025-02-09 06:09:09] 170 loss = 0.00000616, mse = 0.00010414\n",
      "[2025-02-09 06:09:11] 180 loss = 0.00000507, mse = 0.00007855\n",
      "[2025-02-09 06:09:14] 190 loss = 0.00000434, mse = 0.00006263\n",
      "[2025-02-09 06:09:17] 200 loss = 0.00000369, mse = 0.00004764\n",
      "[2025-02-09 06:09:19] 210 loss = 0.00000339, mse = 0.00003920\n",
      "[2025-02-09 06:09:22] 220 loss = 0.00000305, mse = 0.00003263\n",
      "[2025-02-09 06:09:24] 230 loss = 0.00000286, mse = 0.00002729\n",
      "[2025-02-09 06:09:26] 240 loss = 0.00000262, mse = 0.00002246\n",
      "[2025-02-09 06:09:29] 250 loss = 0.00000244, mse = 0.00001883\n",
      "[2025-02-09 06:09:31] 260 loss = 0.00000242, mse = 0.00001589\n",
      "[2025-02-09 06:09:33] 270 loss = 0.00000227, mse = 0.00001498\n",
      "[2025-02-09 06:09:34] 280 loss = 0.00000227, mse = 0.00001498\n",
      "[2025-02-09 06:09:35] 290 loss = 0.00000227, mse = 0.00001498\n",
      "imidx_list: [573]\n",
      "loss_DLG: 2.267666332045337e-06\n",
      "mse_DLG: 1.4982548236730509e-05\n",
      "gt_label: [16] lab_DLG: 16\n",
      "----------------------\n",
      "\n",
      "\n",
      "running 4|10 experiment\n",
      "DLG, Try to generate 1 images\n",
      "lr = 1.0\n",
      "[2025-02-09 06:09:36] 0 loss = 117.35396576, mse = 1.75887299\n",
      "[2025-02-09 06:09:38] 10 loss = 3.96665835, mse = 0.92290020\n",
      "[2025-02-09 06:09:41] 20 loss = 0.52703691, mse = 0.42984328\n",
      "[2025-02-09 06:09:44] 30 loss = 0.11549033, mse = 0.21188472\n",
      "[2025-02-09 06:09:46] 40 loss = 0.03387283, mse = 0.10735246\n",
      "[2025-02-09 06:09:49] 50 loss = 0.01143322, mse = 0.05947608\n",
      "[2025-02-09 06:09:51] 60 loss = 0.00438571, mse = 0.03487007\n",
      "[2025-02-09 06:09:54] 70 loss = 0.00200613, mse = 0.02166245\n",
      "[2025-02-09 06:09:57] 80 loss = 0.00105679, mse = 0.01375277\n",
      "[2025-02-09 06:09:59] 90 loss = 0.00058754, mse = 0.00858734\n",
      "[2025-02-09 06:10:02] 100 loss = 0.00032271, mse = 0.00520583\n",
      "[2025-02-09 06:10:05] 110 loss = 0.00018099, mse = 0.00327349\n",
      "[2025-02-09 06:10:08] 120 loss = 0.00010516, mse = 0.00215594\n",
      "[2025-02-09 06:10:10] 130 loss = 0.00006436, mse = 0.00145941\n",
      "[2025-02-09 06:10:13] 140 loss = 0.00003994, mse = 0.00101409\n",
      "[2025-02-09 06:10:16] 150 loss = 0.00002770, mse = 0.00076742\n",
      "[2025-02-09 06:10:19] 160 loss = 0.00002027, mse = 0.00060026\n",
      "[2025-02-09 06:10:21] 170 loss = 0.00001616, mse = 0.00049709\n",
      "[2025-02-09 06:10:24] 180 loss = 0.00001269, mse = 0.00040696\n",
      "[2025-02-09 06:10:27] 190 loss = 0.00001036, mse = 0.00033288\n",
      "[2025-02-09 06:10:29] 200 loss = 0.00000872, mse = 0.00028376\n",
      "[2025-02-09 06:10:32] 210 loss = 0.00000761, mse = 0.00024896\n",
      "[2025-02-09 06:10:35] 220 loss = 0.00000664, mse = 0.00022523\n",
      "[2025-02-09 06:10:38] 230 loss = 0.00000600, mse = 0.00020207\n",
      "[2025-02-09 06:10:41] 240 loss = 0.00000559, mse = 0.00018021\n",
      "[2025-02-09 06:10:43] 250 loss = 0.00000517, mse = 0.00016266\n",
      "[2025-02-09 06:10:46] 260 loss = 0.00000470, mse = 0.00014550\n",
      "[2025-02-09 06:10:49] 270 loss = 0.00000427, mse = 0.00013328\n",
      "[2025-02-09 06:10:52] 280 loss = 0.00000424, mse = 0.00012113\n",
      "[2025-02-09 06:10:55] 290 loss = 0.00000377, mse = 0.00010939\n",
      "imidx_list: [48198]\n",
      "loss_DLG: 3.665709527922445e-06\n",
      "mse_DLG: 0.00010120586375705898\n",
      "gt_label: [8] lab_DLG: 8\n",
      "----------------------\n",
      "\n",
      "\n",
      "running 5|10 experiment\n",
      "DLG, Try to generate 1 images\n",
      "lr = 1.0\n",
      "[2025-02-09 06:10:58] 0 loss = 44.11315536, mse = 1.38191485\n",
      "[2025-02-09 06:11:00] 10 loss = 0.72244370, mse = 0.40038046\n",
      "[2025-02-09 06:11:03] 20 loss = 0.07512959, mse = 0.14137208\n",
      "[2025-02-09 06:11:05] 30 loss = 0.01471736, mse = 0.05865769\n",
      "[2025-02-09 06:11:08] 40 loss = 0.00435499, mse = 0.02685349\n",
      "[2025-02-09 06:11:10] 50 loss = 0.00139417, mse = 0.01126843\n",
      "[2025-02-09 06:11:13] 60 loss = 0.00053656, mse = 0.00536585\n",
      "[2025-02-09 06:11:16] 70 loss = 0.00023295, mse = 0.00255742\n",
      "[2025-02-09 06:11:19] 80 loss = 0.00009945, mse = 0.00116914\n",
      "[2025-02-09 06:11:21] 90 loss = 0.00004881, mse = 0.00059038\n",
      "[2025-02-09 06:11:24] 100 loss = 0.00002417, mse = 0.00029046\n",
      "[2025-02-09 06:11:27] 110 loss = 0.00001261, mse = 0.00012885\n",
      "[2025-02-09 06:11:29] 120 loss = 0.00000717, mse = 0.00005515\n",
      "[2025-02-09 06:11:31] 130 loss = 0.00000503, mse = 0.00002971\n",
      "[2025-02-09 06:11:34] 140 loss = 0.00000368, mse = 0.00001496\n",
      "[2025-02-09 06:11:35] 150 loss = 0.00000330, mse = 0.00001127\n",
      "[2025-02-09 06:11:37] 160 loss = 0.00000301, mse = 0.00000824\n",
      "[2025-02-09 06:11:38] 170 loss = 0.00000279, mse = 0.00000630\n",
      "[2025-02-09 06:11:40] 180 loss = 0.00000265, mse = 0.00000500\n",
      "[2025-02-09 06:11:41] 190 loss = 0.00000253, mse = 0.00000425\n",
      "[2025-02-09 06:11:42] 200 loss = 0.00000243, mse = 0.00000342\n",
      "[2025-02-09 06:11:44] 210 loss = 0.00000229, mse = 0.00000262\n",
      "[2025-02-09 06:11:46] 220 loss = 0.00000222, mse = 0.00000226\n",
      "[2025-02-09 06:11:47] 230 loss = 0.00000214, mse = 0.00000190\n",
      "[2025-02-09 06:11:48] 240 loss = 0.00000211, mse = 0.00000182\n",
      "[2025-02-09 06:11:49] 250 loss = 0.00000211, mse = 0.00000182\n",
      "[2025-02-09 06:11:50] 260 loss = 0.00000211, mse = 0.00000182\n",
      "[2025-02-09 06:11:51] 270 loss = 0.00000211, mse = 0.00000182\n",
      "[2025-02-09 06:11:52] 280 loss = 0.00000211, mse = 0.00000182\n",
      "[2025-02-09 06:11:53] 290 loss = 0.00000211, mse = 0.00000182\n",
      "imidx_list: [4181]\n",
      "loss_DLG: 2.1082696548546664e-06\n",
      "mse_DLG: 1.818582404666813e-06\n",
      "gt_label: [33] lab_DLG: 33\n",
      "----------------------\n",
      "\n",
      "\n",
      "running 6|10 experiment\n",
      "DLG, Try to generate 1 images\n",
      "lr = 1.0\n",
      "[2025-02-09 06:11:54] 0 loss = 536.61956787, mse = 218.47277832\n",
      "[2025-02-09 06:11:55] 10 loss = 1088.14843750, mse = 4142364416.00000000\n",
      "[2025-02-09 06:11:55] 20 loss = 1088.14843750, mse = 4142364416.00000000\n",
      "[2025-02-09 06:11:55] 30 loss = 1088.14843750, mse = 4142364416.00000000\n",
      "[2025-02-09 06:11:56] 40 loss = 1088.14843750, mse = 4142364416.00000000\n",
      "[2025-02-09 06:11:56] 50 loss = 1088.14843750, mse = 4142364416.00000000\n",
      "[2025-02-09 06:11:56] 60 loss = 1088.14843750, mse = 4142364416.00000000\n",
      "[2025-02-09 06:11:56] 70 loss = 1088.14843750, mse = 4142364416.00000000\n",
      "[2025-02-09 06:11:57] 80 loss = 1088.14843750, mse = 4142364416.00000000\n",
      "[2025-02-09 06:11:57] 90 loss = 1088.14843750, mse = 4142364416.00000000\n",
      "[2025-02-09 06:11:58] 100 loss = 1088.14843750, mse = 4142364416.00000000\n",
      "[2025-02-09 06:11:58] 110 loss = 1088.14843750, mse = 4142364416.00000000\n",
      "[2025-02-09 06:11:59] 120 loss = 1088.14843750, mse = 4142364416.00000000\n",
      "[2025-02-09 06:11:59] 130 loss = 1088.14843750, mse = 4142364416.00000000\n",
      "[2025-02-09 06:12:00] 140 loss = 1088.14843750, mse = 4142364416.00000000\n",
      "[2025-02-09 06:12:00] 150 loss = 1088.14843750, mse = 4142364416.00000000\n",
      "[2025-02-09 06:12:01] 160 loss = 1088.14843750, mse = 4142364416.00000000\n",
      "[2025-02-09 06:12:02] 170 loss = 1088.14843750, mse = 4142364416.00000000\n",
      "[2025-02-09 06:12:03] 180 loss = 1088.14843750, mse = 4142364416.00000000\n",
      "[2025-02-09 06:12:04] 190 loss = 1088.14843750, mse = 4142364416.00000000\n",
      "[2025-02-09 06:12:05] 200 loss = 1088.14843750, mse = 4142364416.00000000\n",
      "[2025-02-09 06:12:06] 210 loss = 1088.14843750, mse = 4142364416.00000000\n",
      "[2025-02-09 06:12:06] 220 loss = 1088.14843750, mse = 4142364416.00000000\n",
      "[2025-02-09 06:12:07] 230 loss = 1088.14843750, mse = 4142364416.00000000\n",
      "[2025-02-09 06:12:08] 240 loss = 1088.14843750, mse = 4142364416.00000000\n",
      "[2025-02-09 06:12:09] 250 loss = 1088.14843750, mse = 4142364416.00000000\n",
      "[2025-02-09 06:12:10] 260 loss = 1088.14843750, mse = 4142364416.00000000\n",
      "[2025-02-09 06:12:11] 270 loss = 1088.14843750, mse = 4142364416.00000000\n",
      "[2025-02-09 06:12:12] 280 loss = 1088.14843750, mse = 4142364416.00000000\n",
      "[2025-02-09 06:12:13] 290 loss = 1088.14843750, mse = 4142364416.00000000\n",
      "imidx_list: [32160]\n",
      "loss_DLG: 1088.1484375\n",
      "mse_DLG: 4142364416.0\n",
      "gt_label: [45] lab_DLG: 60\n",
      "----------------------\n",
      "\n",
      "\n",
      "running 7|10 experiment\n",
      "DLG, Try to generate 1 images\n",
      "lr = 1.0\n",
      "[2025-02-09 06:12:14] 0 loss = 157.11614990, mse = 2.12968802\n",
      "[2025-02-09 06:12:16] 10 loss = 17.36729431, mse = 1.88827169\n",
      "[2025-02-09 06:12:18] 20 loss = 3.25720668, mse = 0.93937540\n",
      "[2025-02-09 06:12:21] 30 loss = 0.97059655, mse = 0.54751152\n",
      "[2025-02-09 06:12:23] 40 loss = 0.28122994, mse = 0.28677404\n",
      "[2025-02-09 06:12:26] 50 loss = 0.07691126, mse = 0.13122663\n",
      "[2025-02-09 06:12:29] 60 loss = 0.02025615, mse = 0.05762210\n",
      "[2025-02-09 06:12:31] 70 loss = 0.00677112, mse = 0.02891736\n",
      "[2025-02-09 06:12:34] 80 loss = 0.00271017, mse = 0.01418306\n",
      "[2025-02-09 06:12:37] 90 loss = 0.00102715, mse = 0.00660855\n",
      "[2025-02-09 06:12:40] 100 loss = 0.00041173, mse = 0.00333674\n",
      "[2025-02-09 06:12:43] 110 loss = 0.00019527, mse = 0.00191706\n",
      "[2025-02-09 06:12:46] 120 loss = 0.00010113, mse = 0.00115426\n",
      "[2025-02-09 06:12:49] 130 loss = 0.00005680, mse = 0.00073760\n",
      "[2025-02-09 06:12:52] 140 loss = 0.00003524, mse = 0.00051245\n",
      "[2025-02-09 06:12:54] 150 loss = 0.00002478, mse = 0.00037526\n",
      "[2025-02-09 06:12:57] 160 loss = 0.00001808, mse = 0.00028412\n",
      "[2025-02-09 06:13:00] 170 loss = 0.00001378, mse = 0.00021427\n",
      "[2025-02-09 06:13:03] 180 loss = 0.00001108, mse = 0.00016678\n",
      "[2025-02-09 06:13:05] 190 loss = 0.00000915, mse = 0.00012771\n",
      "[2025-02-09 06:13:07] 200 loss = 0.00000796, mse = 0.00010573\n",
      "[2025-02-09 06:13:10] 210 loss = 0.00000658, mse = 0.00007962\n",
      "[2025-02-09 06:13:13] 220 loss = 0.00000573, mse = 0.00006491\n",
      "[2025-02-09 06:13:15] 230 loss = 0.00000517, mse = 0.00005447\n",
      "[2025-02-09 06:13:18] 240 loss = 0.00000487, mse = 0.00004964\n",
      "[2025-02-09 06:13:20] 250 loss = 0.00000445, mse = 0.00004269\n",
      "[2025-02-09 06:13:23] 260 loss = 0.00000403, mse = 0.00003585\n",
      "[2025-02-09 06:13:25] 270 loss = 0.00000374, mse = 0.00002954\n",
      "[2025-02-09 06:13:28] 280 loss = 0.00000355, mse = 0.00002670\n",
      "[2025-02-09 06:13:30] 290 loss = 0.00000328, mse = 0.00002217\n",
      "imidx_list: [31151]\n",
      "loss_DLG: 3.1875249533186434e-06\n",
      "mse_DLG: 2.0847161067649722e-05\n",
      "gt_label: [2] lab_DLG: 2\n",
      "----------------------\n",
      "\n",
      "\n",
      "running 8|10 experiment\n",
      "DLG, Try to generate 1 images\n",
      "lr = 1.0\n",
      "[2025-02-09 06:13:33] 0 loss = 35.14855957, mse = 1.23692977\n",
      "[2025-02-09 06:13:36] 10 loss = 0.87026912, mse = 0.72785795\n",
      "[2025-02-09 06:13:38] 20 loss = 0.21763529, mse = 0.38840955\n",
      "[2025-02-09 06:13:41] 30 loss = 0.05873352, mse = 0.18167758\n",
      "[2025-02-09 06:13:43] 40 loss = 0.01431186, mse = 0.08170153\n",
      "[2025-02-09 06:13:46] 50 loss = 0.00437435, mse = 0.03702284\n",
      "[2025-02-09 06:13:48] 60 loss = 0.00154631, mse = 0.01611815\n",
      "[2025-02-09 06:13:51] 70 loss = 0.00058290, mse = 0.00760901\n",
      "[2025-02-09 06:13:53] 80 loss = 0.00024411, mse = 0.00369820\n",
      "[2025-02-09 06:13:56] 90 loss = 0.00010719, mse = 0.00185806\n",
      "[2025-02-09 06:13:59] 100 loss = 0.00005155, mse = 0.00093818\n",
      "[2025-02-09 06:14:02] 110 loss = 0.00002676, mse = 0.00049179\n",
      "[2025-02-09 06:14:05] 120 loss = 0.00001473, mse = 0.00025914\n",
      "[2025-02-09 06:14:07] 130 loss = 0.00000989, mse = 0.00016899\n",
      "[2025-02-09 06:14:10] 140 loss = 0.00000708, mse = 0.00010584\n",
      "[2025-02-09 06:14:12] 150 loss = 0.00000531, mse = 0.00006791\n",
      "[2025-02-09 06:14:14] 160 loss = 0.00000420, mse = 0.00004594\n",
      "[2025-02-09 06:14:17] 170 loss = 0.00000340, mse = 0.00003005\n",
      "[2025-02-09 06:14:19] 180 loss = 0.00000281, mse = 0.00001983\n",
      "[2025-02-09 06:14:22] 190 loss = 0.00000241, mse = 0.00001423\n",
      "[2025-02-09 06:14:25] 200 loss = 0.00000222, mse = 0.00001096\n",
      "[2025-02-09 06:14:26] 210 loss = 0.00000212, mse = 0.00000983\n",
      "[2025-02-09 06:14:27] 220 loss = 0.00000212, mse = 0.00000983\n",
      "[2025-02-09 06:14:28] 230 loss = 0.00000212, mse = 0.00000983\n",
      "[2025-02-09 06:14:29] 240 loss = 0.00000212, mse = 0.00000983\n",
      "[2025-02-09 06:14:30] 250 loss = 0.00000212, mse = 0.00000983\n",
      "[2025-02-09 06:14:31] 260 loss = 0.00000212, mse = 0.00000983\n",
      "[2025-02-09 06:14:32] 270 loss = 0.00000212, mse = 0.00000983\n",
      "[2025-02-09 06:14:33] 280 loss = 0.00000212, mse = 0.00000983\n",
      "[2025-02-09 06:14:34] 290 loss = 0.00000212, mse = 0.00000983\n",
      "imidx_list: [29328]\n",
      "loss_DLG: 2.1239143279672135e-06\n",
      "mse_DLG: 9.82757228484843e-06\n",
      "gt_label: [10] lab_DLG: 10\n",
      "----------------------\n",
      "\n",
      "\n",
      "running 9|10 experiment\n",
      "DLG, Try to generate 1 images\n",
      "lr = 1.0\n",
      "[2025-02-09 06:14:35] 0 loss = 110.62256622, mse = 1.21316195\n",
      "[2025-02-09 06:14:37] 10 loss = 1.65799797, mse = 0.36992002\n",
      "[2025-02-09 06:14:39] 20 loss = 0.08789866, mse = 0.06841961\n",
      "[2025-02-09 06:14:42] 30 loss = 0.01246551, mse = 0.01978691\n",
      "[2025-02-09 06:14:44] 40 loss = 0.00241018, mse = 0.00584406\n",
      "[2025-02-09 06:14:47] 50 loss = 0.00064443, mse = 0.00192270\n",
      "[2025-02-09 06:14:50] 60 loss = 0.00020177, mse = 0.00070869\n",
      "[2025-02-09 06:14:52] 70 loss = 0.00007713, mse = 0.00026872\n",
      "[2025-02-09 06:14:55] 80 loss = 0.00003386, mse = 0.00009573\n",
      "[2025-02-09 06:14:57] 90 loss = 0.00001887, mse = 0.00003838\n",
      "[2025-02-09 06:15:00] 100 loss = 0.00001232, mse = 0.00001592\n",
      "[2025-02-09 06:15:03] 110 loss = 0.00000944, mse = 0.00000745\n",
      "[2025-02-09 06:15:05] 120 loss = 0.00000806, mse = 0.00000401\n",
      "[2025-02-09 06:15:08] 130 loss = 0.00000720, mse = 0.00000250\n",
      "[2025-02-09 06:15:11] 140 loss = 0.00000661, mse = 0.00000184\n",
      "[2025-02-09 06:15:14] 150 loss = 0.00000614, mse = 0.00000151\n",
      "[2025-02-09 06:15:16] 160 loss = 0.00000556, mse = 0.00000188\n",
      "[2025-02-09 06:15:18] 170 loss = 0.00000511, mse = 0.00000238\n",
      "[2025-02-09 06:15:21] 180 loss = 0.00000476, mse = 0.00000288\n",
      "[2025-02-09 06:15:24] 190 loss = 0.00000455, mse = 0.00000316\n",
      "[2025-02-09 06:15:27] 200 loss = 0.00000427, mse = 0.00000361\n",
      "[2025-02-09 06:15:29] 210 loss = 0.00000402, mse = 0.00000368\n",
      "[2025-02-09 06:15:32] 220 loss = 0.00000375, mse = 0.00000361\n",
      "[2025-02-09 06:15:35] 230 loss = 0.00000352, mse = 0.00000336\n",
      "[2025-02-09 06:15:38] 240 loss = 0.00000328, mse = 0.00000316\n",
      "[2025-02-09 06:15:41] 250 loss = 0.00000297, mse = 0.00000275\n",
      "[2025-02-09 06:15:44] 260 loss = 0.00000263, mse = 0.00000251\n",
      "[2025-02-09 06:15:47] 270 loss = 0.00000229, mse = 0.00000221\n",
      "[2025-02-09 06:15:50] 280 loss = 0.00000203, mse = 0.00000190\n",
      "[2025-02-09 06:15:53] 290 loss = 0.00000193, mse = 0.00000161\n",
      "imidx_list: [42688]\n",
      "loss_DLG: 1.6914724483285681e-06\n",
      "mse_DLG: 1.4489537534245756e-06\n",
      "gt_label: [89] lab_DLG: 89\n",
      "----------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "''' train DLG and iDLG '''\n",
    "for idx_net in range(num_exp):\n",
    "    net = LeNet(channel=channel, hideen=hidden, num_classes=num_classes)\n",
    "    net.apply(weights_init)\n",
    "\n",
    "    print('running %d|%d experiment'%(idx_net, num_exp))########\n",
    "    net = net.to(device)\n",
    "    idx_shuffle = np.random.permutation(len(dataset))\n",
    "\n",
    "    print('%s, Try to generate %d images' % (\"DLG\", num_dummy))\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    imidx_list = []\n",
    "\n",
    "    for imidx in range(num_dummy):\n",
    "        idx = idx_shuffle[imidx]\n",
    "        imidx_list.append(idx)\n",
    "        tmp_datum = tt(dataset[idx][0]).float().to(device)\n",
    "        tmp_datum = tmp_datum.view(1, *tmp_datum.size())\n",
    "        tmp_label = torch.Tensor([dataset[idx][1]]).long().to(device)\n",
    "        tmp_label = tmp_label.view(1, )\n",
    "        if imidx == 0:\n",
    "            gt_data = tmp_datum\n",
    "            gt_label = tmp_label\n",
    "        else:\n",
    "            gt_data = torch.cat((gt_data, tmp_datum), dim=0)\n",
    "            gt_label = torch.cat((gt_label, tmp_label), dim=0)\n",
    "\n",
    "\n",
    "    # compute original gradient\n",
    "    out = net(gt_data)\n",
    "    y = criterion(out, gt_label)\n",
    "    dy_dx = torch.autograd.grad(y, net.parameters())\n",
    "    original_dy_dx = list((_.detach().clone() for _ in dy_dx))\n",
    "\n",
    "    # generate dummy data and label\n",
    "    dummy_data = torch.randn(gt_data.size()).to(device).requires_grad_(True)\n",
    "    dummy_label = torch.randn((gt_data.shape[0], num_classes)).to(device).requires_grad_(True)\n",
    "\n",
    "    optimizer = torch.optim.LBFGS([dummy_data, dummy_label], lr=lr)\n",
    "\n",
    "\n",
    "    history = []\n",
    "    history_iters = []\n",
    "    losses = []\n",
    "    mses = []\n",
    "    train_iters = []\n",
    "\n",
    "    print('lr =', lr)\n",
    "    for iters in range(Iteration):\n",
    "\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            pred = net(dummy_data)\n",
    "\n",
    "            dummy_loss = - torch.mean(torch.sum(torch.softmax(dummy_label, -1) * torch.log(torch.softmax(pred, -1)), dim=-1))\n",
    "            # dummy_loss = criterion(pred, gt_label)\n",
    "\n",
    "\n",
    "            dummy_dy_dx = torch.autograd.grad(dummy_loss, net.parameters(), create_graph=True)\n",
    "\n",
    "            grad_diff = 0\n",
    "            for gx, gy in zip(dummy_dy_dx, original_dy_dx):\n",
    "                grad_diff += ((gx - gy) ** 2).sum()\n",
    "            grad_diff.backward()\n",
    "            return grad_diff\n",
    "\n",
    "        optimizer.step(closure)\n",
    "        current_loss = closure().item()\n",
    "        train_iters.append(iters)\n",
    "        losses.append(current_loss)\n",
    "        mses.append(torch.mean((dummy_data-gt_data)**2).item())\n",
    "\n",
    "\n",
    "        if iters % int(Iteration / 30) == 0:\n",
    "            current_time = str(time.strftime(\"[%Y-%m-%d %H:%M:%S]\", time.localtime()))\n",
    "            print(current_time, iters, 'loss = %.8f, mse = %.8f' %(current_loss, mses[-1]))\n",
    "            history.append([tp(dummy_data[imidx].cpu()) for imidx in range(num_dummy)])\n",
    "            history_iters.append(iters)\n",
    "\n",
    "            for imidx in range(num_dummy):\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                plt.subplot(3, 10, 1)\n",
    "                plt.imshow(tp(gt_data[imidx].cpu()))\n",
    "                for i in range(min(len(history), 29)):\n",
    "                    plt.subplot(3, 10, i + 2)\n",
    "                    plt.imshow(history[i][imidx])\n",
    "                    plt.title('iter=%d' % (history_iters[i]))\n",
    "                    plt.axis('off')\n",
    "\n",
    "                    plt.savefig('%s/DLG_on_%s_%05d.png' % (save_path, imidx_list, imidx_list[imidx]))\n",
    "                    plt.close()\n",
    "\n",
    "\n",
    "            if current_loss < 0.000001: # converge\n",
    "                break\n",
    "\n",
    "    loss_DLG = losses\n",
    "    label_DLG = torch.argmax(dummy_label, dim=-1).detach().item()\n",
    "    mse_DLG = mses\n",
    "\n",
    "\n",
    "    print('imidx_list:', imidx_list)\n",
    "    print('loss_DLG:', loss_DLG[-1], )\n",
    "    print('mse_DLG:', mse_DLG[-1])\n",
    "    print('gt_label:', gt_label.detach().cpu().data.numpy(), 'lab_DLG:', label_DLG, )\n",
    "\n",
    "    print('----------------------\\n\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
