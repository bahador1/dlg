{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import pickle\n",
    "import PIL.Image as Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Running on cuda\n"
     ]
    }
   ],
   "source": [
    "dst = datasets.CIFAR100(\"../Data\", download=True)\n",
    "tp = transforms.Compose([\n",
    "    transforms.Resize(32),\n",
    "    transforms.CenterCrop(32),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "tt = transforms.ToPILImage()\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "print(\"Running on %s\" % device)\n",
    "\n",
    "def label_to_onehot(target, num_classes=100):\n",
    "    target = torch.unsqueeze(target, 1)\n",
    "    onehot_target = torch.zeros(target.size(0), num_classes, device=target.device)\n",
    "    onehot_target.scatter_(1, target, 1)\n",
    "    return onehot_target\n",
    "\n",
    "def cross_entropy_for_onehot(pred, target):\n",
    "    return torch.mean(torch.sum(- target * F.log_softmax(pred, dim=-1), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self, channel=3, hideen=768, num_classes=10):\n",
    "        super(LeNet, self).__init__()\n",
    "        act = nn.Sigmoid\n",
    "        self.body = nn.Sequential(\n",
    "            nn.Conv2d(channel, 12, kernel_size=5, padding=5 // 2, stride=2),\n",
    "            act(),\n",
    "            nn.Conv2d(12, 12, kernel_size=5, padding=5 // 2, stride=2),\n",
    "            act(),\n",
    "            nn.Conv2d(12, 12, kernel_size=5, padding=5 // 2, stride=1),\n",
    "            act(),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(hideen, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.body(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    try:\n",
    "        if hasattr(m, \"weight\"):\n",
    "            m.weight.data.uniform_(-0.5, 0.5)\n",
    "    except Exception:\n",
    "        print('warning: failed in weights_init for %s.weight' % m._get_name())\n",
    "    try:\n",
    "        if hasattr(m, \"bias\"):\n",
    "            m.bias.data.uniform_(-0.5, 0.5)\n",
    "    except Exception:\n",
    "        print('warning: failed in weights_init for %s.bias' % m._get_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cifar100 root_path: .\n",
      "cifar100 data_path: ./../Data\n",
      "cifar100 save_path: ./results/DLG_cifar100\n",
      "Files already downloaded and verified\n",
      "running 0|10 experiment\n",
      "DLG, Try to generate 1 images\n",
      "lr = 1.0\n",
      "[2025-02-09 01:51:29] 0 loss = 44.87969971, mse = 1.47502518\n",
      "[2025-02-09 01:51:31] 10 loss = 0.66931063, mse = 0.31633073\n",
      "[2025-02-09 01:51:34] 20 loss = 0.02810415, mse = 0.05861599\n",
      "[2025-02-09 01:51:37] 30 loss = 0.00406719, mse = 0.01615308\n",
      "[2025-02-09 01:51:39] 40 loss = 0.00091327, mse = 0.00555546\n",
      "[2025-02-09 01:51:42] 50 loss = 0.00024812, mse = 0.00180916\n",
      "[2025-02-09 01:51:45] 60 loss = 0.00007280, mse = 0.00058133\n",
      "[2025-02-09 01:51:48] 70 loss = 0.00002531, mse = 0.00018680\n",
      "[2025-02-09 01:51:50] 80 loss = 0.00001049, mse = 0.00006045\n",
      "[2025-02-09 01:51:53] 90 loss = 0.00000554, mse = 0.00002143\n",
      "[2025-02-09 01:51:55] 100 loss = 0.00000396, mse = 0.00001121\n",
      "[2025-02-09 01:51:58] 110 loss = 0.00000323, mse = 0.00000610\n",
      "[2025-02-09 01:52:00] 120 loss = 0.00000293, mse = 0.00000393\n",
      "[2025-02-09 01:52:02] 130 loss = 0.00000268, mse = 0.00000268\n",
      "[2025-02-09 01:52:04] 140 loss = 0.00000253, mse = 0.00000178\n",
      "[2025-02-09 01:52:06] 150 loss = 0.00000235, mse = 0.00000120\n",
      "[2025-02-09 01:52:08] 160 loss = 0.00000221, mse = 0.00000089\n",
      "[2025-02-09 01:52:10] 170 loss = 0.00000210, mse = 0.00000081\n",
      "[2025-02-09 01:52:13] 180 loss = 0.00000199, mse = 0.00000091\n",
      "[2025-02-09 01:52:14] 190 loss = 0.00000200, mse = 0.00000091\n",
      "[2025-02-09 01:52:15] 200 loss = 0.00000200, mse = 0.00000091\n",
      "[2025-02-09 01:52:16] 210 loss = 0.00000200, mse = 0.00000091\n",
      "[2025-02-09 01:52:17] 220 loss = 0.00000200, mse = 0.00000091\n",
      "[2025-02-09 01:52:17] 230 loss = 0.00000200, mse = 0.00000091\n",
      "[2025-02-09 01:52:18] 240 loss = 0.00000200, mse = 0.00000091\n",
      "[2025-02-09 01:52:19] 250 loss = 0.00000200, mse = 0.00000091\n",
      "[2025-02-09 01:52:20] 260 loss = 0.00000200, mse = 0.00000091\n",
      "[2025-02-09 01:52:21] 270 loss = 0.00000200, mse = 0.00000091\n",
      "[2025-02-09 01:52:22] 280 loss = 0.00000200, mse = 0.00000091\n",
      "[2025-02-09 01:52:23] 290 loss = 0.00000200, mse = 0.00000091\n",
      "imidx_list: [45475]\n",
      "loss_DLG: 2.0032293832628056e-06\n",
      "mse_DLG: 9.128189049079083e-07\n",
      "gt_label: [3] lab_DLG: 3\n",
      "----------------------\n",
      "\n",
      "\n",
      "running 1|10 experiment\n",
      "DLG, Try to generate 1 images\n",
      "lr = 1.0\n",
      "[2025-02-09 01:52:25] 0 loss = 64.34645081, mse = 1.51632535\n",
      "[2025-02-09 01:52:27] 10 loss = 0.57237709, mse = 0.61056900\n",
      "[2025-02-09 01:52:29] 20 loss = 0.05029207, mse = 0.26629719\n",
      "[2025-02-09 01:52:32] 30 loss = 0.01235063, mse = 0.14194286\n",
      "[2025-02-09 01:52:34] 40 loss = 0.00391487, mse = 0.08498500\n",
      "[2025-02-09 01:52:37] 50 loss = 0.00173180, mse = 0.05545480\n",
      "[2025-02-09 01:52:40] 60 loss = 0.00083975, mse = 0.03598133\n",
      "[2025-02-09 01:52:43] 70 loss = 0.00042968, mse = 0.02315252\n",
      "[2025-02-09 01:52:45] 80 loss = 0.00022770, mse = 0.01469283\n",
      "[2025-02-09 01:52:48] 90 loss = 0.00013543, mse = 0.00967387\n",
      "[2025-02-09 01:52:51] 100 loss = 0.00007987, mse = 0.00609489\n",
      "[2025-02-09 01:52:54] 110 loss = 0.00004512, mse = 0.00362772\n",
      "[2025-02-09 01:52:57] 120 loss = 0.00002739, mse = 0.00226927\n",
      "[2025-02-09 01:53:00] 130 loss = 0.00001611, mse = 0.00137037\n",
      "[2025-02-09 01:53:03] 140 loss = 0.00001002, mse = 0.00088108\n",
      "[2025-02-09 01:53:06] 150 loss = 0.00000646, mse = 0.00058264\n",
      "[2025-02-09 01:53:08] 160 loss = 0.00000477, mse = 0.00043142\n",
      "[2025-02-09 01:53:10] 170 loss = 0.00000398, mse = 0.00036372\n",
      "[2025-02-09 01:53:12] 180 loss = 0.00000342, mse = 0.00029889\n",
      "[2025-02-09 01:53:14] 190 loss = 0.00000300, mse = 0.00025596\n",
      "[2025-02-09 01:53:16] 200 loss = 0.00000253, mse = 0.00020909\n",
      "[2025-02-09 01:53:18] 210 loss = 0.00000208, mse = 0.00016736\n",
      "[2025-02-09 01:53:20] 220 loss = 0.00000189, mse = 0.00014871\n",
      "[2025-02-09 01:53:21] 230 loss = 0.00000174, mse = 0.00013535\n",
      "[2025-02-09 01:53:23] 240 loss = 0.00000153, mse = 0.00011645\n",
      "[2025-02-09 01:53:25] 250 loss = 0.00000141, mse = 0.00010438\n",
      "[2025-02-09 01:53:27] 260 loss = 0.00000129, mse = 0.00009247\n",
      "[2025-02-09 01:53:28] 270 loss = 0.00000122, mse = 0.00008720\n",
      "[2025-02-09 01:53:30] 280 loss = 0.00000115, mse = 0.00008087\n",
      "[2025-02-09 01:53:32] 290 loss = 0.00000109, mse = 0.00007402\n",
      "imidx_list: [14681]\n",
      "loss_DLG: 1.067511448127334e-06\n",
      "mse_DLG: 7.201352127594873e-05\n",
      "gt_label: [25] lab_DLG: 25\n",
      "----------------------\n",
      "\n",
      "\n",
      "running 2|10 experiment\n",
      "DLG, Try to generate 1 images\n",
      "lr = 1.0\n",
      "[2025-02-09 01:53:33] 0 loss = 46.54465485, mse = 2.00466442\n",
      "[2025-02-09 01:53:35] 10 loss = 1.27708173, mse = 0.73242795\n",
      "[2025-02-09 01:53:37] 20 loss = 0.11353391, mse = 0.29128873\n",
      "[2025-02-09 01:53:40] 30 loss = 0.02676044, mse = 0.13488802\n",
      "[2025-02-09 01:53:43] 40 loss = 0.00738224, mse = 0.06124155\n",
      "[2025-02-09 01:53:46] 50 loss = 0.00233636, mse = 0.02874477\n",
      "[2025-02-09 01:53:49] 60 loss = 0.00086393, mse = 0.01359841\n",
      "[2025-02-09 01:53:51] 70 loss = 0.00032588, mse = 0.00636528\n",
      "[2025-02-09 01:53:54] 80 loss = 0.00015340, mse = 0.00341215\n",
      "[2025-02-09 01:53:57] 90 loss = 0.00007089, mse = 0.00165700\n",
      "[2025-02-09 01:54:00] 100 loss = 0.00003057, mse = 0.00071310\n",
      "[2025-02-09 01:54:02] 110 loss = 0.00001580, mse = 0.00035848\n",
      "[2025-02-09 01:54:05] 120 loss = 0.00000850, mse = 0.00017497\n",
      "[2025-02-09 01:54:08] 130 loss = 0.00000511, mse = 0.00008365\n",
      "[2025-02-09 01:54:10] 140 loss = 0.00000344, mse = 0.00004689\n",
      "[2025-02-09 01:54:13] 150 loss = 0.00000271, mse = 0.00003084\n",
      "[2025-02-09 01:54:16] 160 loss = 0.00000212, mse = 0.00002094\n",
      "[2025-02-09 01:54:19] 170 loss = 0.00000187, mse = 0.00001528\n",
      "[2025-02-09 01:54:22] 180 loss = 0.00000174, mse = 0.00001220\n",
      "[2025-02-09 01:54:25] 190 loss = 0.00000173, mse = 0.00000962\n",
      "[2025-02-09 01:54:27] 200 loss = 0.00000157, mse = 0.00000877\n",
      "[2025-02-09 01:54:28] 210 loss = 0.00000157, mse = 0.00000877\n",
      "[2025-02-09 01:54:28] 220 loss = 0.00000157, mse = 0.00000877\n",
      "[2025-02-09 01:54:29] 230 loss = 0.00000157, mse = 0.00000877\n",
      "[2025-02-09 01:54:30] 240 loss = 0.00000157, mse = 0.00000877\n",
      "[2025-02-09 01:54:31] 250 loss = 0.00000157, mse = 0.00000877\n",
      "[2025-02-09 01:54:33] 260 loss = 0.00000157, mse = 0.00000877\n",
      "[2025-02-09 01:54:34] 270 loss = 0.00000157, mse = 0.00000877\n",
      "[2025-02-09 01:54:35] 280 loss = 0.00000157, mse = 0.00000877\n",
      "[2025-02-09 01:54:36] 290 loss = 0.00000157, mse = 0.00000877\n",
      "imidx_list: [6947]\n",
      "loss_DLG: 1.5688279972891905e-06\n",
      "mse_DLG: 8.76705780683551e-06\n",
      "gt_label: [32] lab_DLG: 32\n",
      "----------------------\n",
      "\n",
      "\n",
      "running 3|10 experiment\n",
      "DLG, Try to generate 1 images\n",
      "lr = 1.0\n",
      "[2025-02-09 01:54:38] 0 loss = 241.26011658, mse = 11.08088207\n",
      "[2025-02-09 01:54:40] 10 loss = 54.12574768, mse = 14.52367973\n",
      "[2025-02-09 01:54:42] 20 loss = 31.69075584, mse = 11.78758144\n",
      "[2025-02-09 01:54:45] 30 loss = 22.41959381, mse = 11.34686852\n",
      "[2025-02-09 01:54:48] 40 loss = 17.10914803, mse = 11.92366028\n",
      "[2025-02-09 01:54:51] 50 loss = 14.63020515, mse = 12.76427078\n",
      "[2025-02-09 01:54:53] 60 loss = 12.65408134, mse = 14.03986263\n",
      "[2025-02-09 01:54:56] 70 loss = 10.47970676, mse = 14.84052563\n",
      "[2025-02-09 01:54:58] 80 loss = 8.91109657, mse = 15.86643505\n",
      "[2025-02-09 01:55:01] 90 loss = 7.66898441, mse = 16.54954147\n",
      "[2025-02-09 01:55:04] 100 loss = 7.05502033, mse = 17.43487930\n",
      "[2025-02-09 01:55:06] 110 loss = 6.50726509, mse = 18.58473015\n",
      "[2025-02-09 01:55:09] 120 loss = 6.10859489, mse = 19.75921440\n",
      "[2025-02-09 01:55:12] 130 loss = 5.84138775, mse = 20.90831184\n",
      "[2025-02-09 01:55:15] 140 loss = 5.30453825, mse = 21.97082138\n",
      "[2025-02-09 01:55:17] 150 loss = 5.08305740, mse = 22.93726921\n",
      "[2025-02-09 01:55:20] 160 loss = 4.85942698, mse = 24.29025078\n",
      "[2025-02-09 01:55:23] 170 loss = 4.65208340, mse = 25.39870453\n",
      "[2025-02-09 01:55:26] 180 loss = 4.47290373, mse = 26.62707138\n",
      "[2025-02-09 01:55:29] 190 loss = 4.11094761, mse = 27.55210686\n",
      "[2025-02-09 01:55:32] 200 loss = 3.80095243, mse = 27.90131760\n",
      "[2025-02-09 01:55:35] 210 loss = 3.65353251, mse = 28.65496063\n",
      "[2025-02-09 01:55:38] 220 loss = 3.49259138, mse = 29.59792709\n",
      "[2025-02-09 01:55:41] 230 loss = 3.38335133, mse = 30.33988571\n",
      "[2025-02-09 01:55:44] 240 loss = 3.30014014, mse = 30.96251488\n",
      "[2025-02-09 01:55:48] 250 loss = 3.22775769, mse = 31.78271484\n",
      "[2025-02-09 01:55:51] 260 loss = 3.14859819, mse = 32.71428680\n",
      "[2025-02-09 01:55:54] 270 loss = 3.08929467, mse = 33.66218567\n",
      "[2025-02-09 01:55:57] 280 loss = 3.02855802, mse = 34.85134125\n",
      "[2025-02-09 01:56:01] 290 loss = 2.95915318, mse = 35.81657410\n",
      "imidx_list: [46975]\n",
      "loss_DLG: 2.907604455947876\n",
      "mse_DLG: 36.541561126708984\n",
      "gt_label: [37] lab_DLG: 37\n",
      "----------------------\n",
      "\n",
      "\n",
      "running 4|10 experiment\n",
      "DLG, Try to generate 1 images\n",
      "lr = 1.0\n",
      "[2025-02-09 01:56:04] 0 loss = 117.55049133, mse = 2.11605597\n",
      "[2025-02-09 01:56:06] 10 loss = 6.92860126, mse = 1.47618520\n",
      "[2025-02-09 01:56:08] 20 loss = 2.20082402, mse = 0.97876930\n",
      "[2025-02-09 01:56:11] 30 loss = 0.94904995, mse = 0.61023080\n",
      "[2025-02-09 01:56:14] 40 loss = 0.29943469, mse = 0.28259826\n",
      "[2025-02-09 01:56:16] 50 loss = 0.06877879, mse = 0.10635465\n",
      "[2025-02-09 01:56:19] 60 loss = 0.01741486, mse = 0.03812740\n",
      "[2025-02-09 01:56:22] 70 loss = 0.00428536, mse = 0.01309715\n",
      "[2025-02-09 01:56:25] 80 loss = 0.00124828, mse = 0.00489205\n",
      "[2025-02-09 01:56:27] 90 loss = 0.00039893, mse = 0.00170691\n",
      "[2025-02-09 01:56:30] 100 loss = 0.00013234, mse = 0.00058209\n",
      "[2025-02-09 01:56:33] 110 loss = 0.00004309, mse = 0.00021025\n",
      "[2025-02-09 01:56:35] 120 loss = 0.00001842, mse = 0.00009930\n",
      "[2025-02-09 01:56:38] 130 loss = 0.00000838, mse = 0.00004961\n",
      "[2025-02-09 01:56:40] 140 loss = 0.00000510, mse = 0.00003116\n",
      "[2025-02-09 01:56:43] 150 loss = 0.00000310, mse = 0.00001958\n",
      "[2025-02-09 01:56:45] 160 loss = 0.00000218, mse = 0.00001345\n",
      "[2025-02-09 01:56:48] 170 loss = 0.00000161, mse = 0.00000986\n",
      "[2025-02-09 01:56:51] 180 loss = 0.00000111, mse = 0.00000650\n",
      "[2025-02-09 01:56:54] 190 loss = 0.00000090, mse = 0.00000455\n",
      "imidx_list: [34527]\n",
      "loss_DLG: 9.023202096614114e-07\n",
      "mse_DLG: 4.551527581497794e-06\n",
      "gt_label: [24] lab_DLG: 24\n",
      "----------------------\n",
      "\n",
      "\n",
      "running 5|10 experiment\n",
      "DLG, Try to generate 1 images\n",
      "lr = 1.0\n",
      "[2025-02-09 01:56:55] 0 loss = 44.02166748, mse = 1.50065589\n",
      "[2025-02-09 01:56:57] 10 loss = 1.32342708, mse = 0.42441529\n",
      "[2025-02-09 01:56:59] 20 loss = 0.08571111, mse = 0.08985879\n",
      "[2025-02-09 01:57:02] 30 loss = 0.01130435, mse = 0.02226428\n",
      "[2025-02-09 01:57:04] 40 loss = 0.00219695, mse = 0.00596740\n",
      "[2025-02-09 01:57:07] 50 loss = 0.00045032, mse = 0.00179297\n",
      "[2025-02-09 01:57:09] 60 loss = 0.00014047, mse = 0.00065546\n",
      "[2025-02-09 01:57:12] 70 loss = 0.00004919, mse = 0.00021596\n",
      "[2025-02-09 01:57:15] 80 loss = 0.00002170, mse = 0.00007602\n",
      "[2025-02-09 01:57:17] 90 loss = 0.00001100, mse = 0.00002356\n",
      "[2025-02-09 01:57:19] 100 loss = 0.00000761, mse = 0.00001057\n",
      "[2025-02-09 01:57:21] 110 loss = 0.00000631, mse = 0.00000616\n",
      "[2025-02-09 01:57:23] 120 loss = 0.00000548, mse = 0.00000387\n",
      "[2025-02-09 01:57:26] 130 loss = 0.00000484, mse = 0.00000259\n",
      "[2025-02-09 01:57:27] 140 loss = 0.00000442, mse = 0.00000232\n",
      "[2025-02-09 01:57:30] 150 loss = 0.00000400, mse = 0.00000213\n",
      "[2025-02-09 01:57:32] 160 loss = 0.00000349, mse = 0.00000259\n",
      "[2025-02-09 01:57:34] 170 loss = 0.00000327, mse = 0.00000289\n",
      "[2025-02-09 01:57:36] 180 loss = 0.00000291, mse = 0.00000342\n",
      "[2025-02-09 01:57:38] 190 loss = 0.00000258, mse = 0.00000396\n",
      "[2025-02-09 01:57:41] 200 loss = 0.00000227, mse = 0.00000403\n",
      "[2025-02-09 01:57:43] 210 loss = 0.00000211, mse = 0.00000423\n",
      "[2025-02-09 01:57:46] 220 loss = 0.00000176, mse = 0.00000435\n",
      "[2025-02-09 01:57:48] 230 loss = 0.00000155, mse = 0.00000428\n",
      "[2025-02-09 01:57:51] 240 loss = 0.00000134, mse = 0.00000405\n",
      "[2025-02-09 01:57:52] 250 loss = 0.00000133, mse = 0.00000404\n",
      "[2025-02-09 01:57:53] 260 loss = 0.00000133, mse = 0.00000404\n",
      "[2025-02-09 01:57:54] 270 loss = 0.00000133, mse = 0.00000404\n",
      "[2025-02-09 01:57:55] 280 loss = 0.00000133, mse = 0.00000404\n",
      "[2025-02-09 01:57:56] 290 loss = 0.00000133, mse = 0.00000404\n",
      "imidx_list: [25463]\n",
      "loss_DLG: 1.331490011580172e-06\n",
      "mse_DLG: 4.040053227072349e-06\n",
      "gt_label: [67] lab_DLG: 67\n",
      "----------------------\n",
      "\n",
      "\n",
      "running 6|10 experiment\n",
      "DLG, Try to generate 1 images\n",
      "lr = 1.0\n",
      "[2025-02-09 01:57:57] 0 loss = 37.27929688, mse = 1.40992486\n",
      "[2025-02-09 01:58:00] 10 loss = 0.87522697, mse = 0.63682288\n",
      "[2025-02-09 01:58:02] 20 loss = 0.12733769, mse = 0.30468696\n",
      "[2025-02-09 01:58:05] 30 loss = 0.03738920, mse = 0.16582707\n",
      "[2025-02-09 01:58:08] 40 loss = 0.01151979, mse = 0.08915170\n",
      "[2025-02-09 01:58:10] 50 loss = 0.00427863, mse = 0.04943212\n",
      "[2025-02-09 01:58:13] 60 loss = 0.00186115, mse = 0.02802159\n",
      "[2025-02-09 01:58:16] 70 loss = 0.00086085, mse = 0.01534256\n",
      "[2025-02-09 01:58:18] 80 loss = 0.00042553, mse = 0.00844632\n",
      "[2025-02-09 01:58:22] 90 loss = 0.00019011, mse = 0.00436670\n",
      "[2025-02-09 01:58:25] 100 loss = 0.00009655, mse = 0.00265770\n",
      "[2025-02-09 01:58:28] 110 loss = 0.00005559, mse = 0.00165360\n",
      "[2025-02-09 01:58:31] 120 loss = 0.00003091, mse = 0.00092190\n",
      "[2025-02-09 01:58:33] 130 loss = 0.00001658, mse = 0.00048486\n",
      "[2025-02-09 01:58:36] 140 loss = 0.00001060, mse = 0.00031450\n",
      "[2025-02-09 01:58:39] 150 loss = 0.00000734, mse = 0.00021116\n",
      "[2025-02-09 01:58:41] 160 loss = 0.00000549, mse = 0.00014633\n",
      "[2025-02-09 01:58:44] 170 loss = 0.00000410, mse = 0.00009433\n",
      "[2025-02-09 01:58:47] 180 loss = 0.00000319, mse = 0.00006365\n",
      "[2025-02-09 01:58:49] 190 loss = 0.00000257, mse = 0.00004454\n",
      "[2025-02-09 01:58:52] 200 loss = 0.00000213, mse = 0.00003186\n",
      "[2025-02-09 01:58:55] 210 loss = 0.00000182, mse = 0.00002402\n",
      "[2025-02-09 01:58:58] 220 loss = 0.00000157, mse = 0.00001806\n",
      "[2025-02-09 01:59:00] 230 loss = 0.00000142, mse = 0.00001473\n",
      "[2025-02-09 01:59:03] 240 loss = 0.00000132, mse = 0.00001221\n",
      "[2025-02-09 01:59:04] 250 loss = 0.00000132, mse = 0.00001221\n",
      "[2025-02-09 01:59:05] 260 loss = 0.00000132, mse = 0.00001221\n",
      "[2025-02-09 01:59:06] 270 loss = 0.00000132, mse = 0.00001221\n",
      "[2025-02-09 01:59:07] 280 loss = 0.00000132, mse = 0.00001221\n",
      "[2025-02-09 01:59:09] 290 loss = 0.00000132, mse = 0.00001221\n",
      "imidx_list: [19114]\n",
      "loss_DLG: 1.3167410770620336e-06\n",
      "mse_DLG: 1.2209686246933416e-05\n",
      "gt_label: [1] lab_DLG: 1\n",
      "----------------------\n",
      "\n",
      "\n",
      "running 7|10 experiment\n",
      "DLG, Try to generate 1 images\n",
      "lr = 1.0\n",
      "[2025-02-09 01:59:10] 0 loss = 71.72877502, mse = 1.07892489\n",
      "[2025-02-09 01:59:12] 10 loss = 1.12728083, mse = 0.26507962\n",
      "[2025-02-09 01:59:15] 20 loss = 0.03998756, mse = 0.03153377\n",
      "[2025-02-09 01:59:17] 30 loss = 0.00355630, mse = 0.00569510\n",
      "[2025-02-09 01:59:20] 40 loss = 0.00058461, mse = 0.00123058\n",
      "[2025-02-09 01:59:23] 50 loss = 0.00013345, mse = 0.00032175\n",
      "[2025-02-09 01:59:25] 60 loss = 0.00004213, mse = 0.00009034\n",
      "[2025-02-09 01:59:28] 70 loss = 0.00002049, mse = 0.00003086\n",
      "[2025-02-09 01:59:31] 80 loss = 0.00001424, mse = 0.00001360\n",
      "[2025-02-09 01:59:33] 90 loss = 0.00001081, mse = 0.00000580\n",
      "[2025-02-09 01:59:35] 100 loss = 0.00000862, mse = 0.00000321\n",
      "[2025-02-09 01:59:38] 110 loss = 0.00000737, mse = 0.00000314\n",
      "[2025-02-09 01:59:40] 120 loss = 0.00000645, mse = 0.00000290\n",
      "[2025-02-09 01:59:42] 130 loss = 0.00000593, mse = 0.00000294\n",
      "[2025-02-09 01:59:45] 140 loss = 0.00000520, mse = 0.00000306\n",
      "[2025-02-09 01:59:48] 150 loss = 0.00000468, mse = 0.00000305\n",
      "[2025-02-09 01:59:50] 160 loss = 0.00000411, mse = 0.00000321\n",
      "[2025-02-09 01:59:53] 170 loss = 0.00000358, mse = 0.00000326\n",
      "[2025-02-09 01:59:56] 180 loss = 0.00000295, mse = 0.00000311\n",
      "[2025-02-09 01:59:59] 190 loss = 0.00000249, mse = 0.00000341\n",
      "[2025-02-09 02:00:02] 200 loss = 0.00000219, mse = 0.00000338\n",
      "[2025-02-09 02:00:05] 210 loss = 0.00000195, mse = 0.00000340\n",
      "[2025-02-09 02:00:10] 220 loss = 0.00000177, mse = 0.00000328\n",
      "[2025-02-09 02:00:12] 230 loss = 0.00000173, mse = 0.00000313\n",
      "[2025-02-09 02:00:15] 240 loss = 0.00000155, mse = 0.00000297\n",
      "[2025-02-09 02:00:18] 250 loss = 0.00000142, mse = 0.00000273\n",
      "[2025-02-09 02:00:20] 260 loss = 0.00000132, mse = 0.00000263\n",
      "[2025-02-09 02:00:21] 270 loss = 0.00000132, mse = 0.00000263\n",
      "[2025-02-09 02:00:22] 280 loss = 0.00000132, mse = 0.00000263\n",
      "[2025-02-09 02:00:23] 290 loss = 0.00000132, mse = 0.00000263\n",
      "imidx_list: [33030]\n",
      "loss_DLG: 1.3179916322769714e-06\n",
      "mse_DLG: 2.625044999149395e-06\n",
      "gt_label: [92] lab_DLG: 92\n",
      "----------------------\n",
      "\n",
      "\n",
      "running 8|10 experiment\n",
      "DLG, Try to generate 1 images\n",
      "lr = 1.0\n",
      "[2025-02-09 02:00:24] 0 loss = 513.54974365, mse = 1.31223941\n",
      "[2025-02-09 02:00:25] 10 loss = 1994.46630859, mse = 21700196352.00000000\n",
      "[2025-02-09 02:00:25] 20 loss = 1994.46630859, mse = 21700196352.00000000\n",
      "[2025-02-09 02:00:25] 30 loss = 1994.46630859, mse = 21700196352.00000000\n",
      "[2025-02-09 02:00:26] 40 loss = 1994.46630859, mse = 21700196352.00000000\n",
      "[2025-02-09 02:00:26] 50 loss = 1994.46630859, mse = 21700196352.00000000\n",
      "[2025-02-09 02:00:26] 60 loss = 1994.46630859, mse = 21700196352.00000000\n",
      "[2025-02-09 02:00:27] 70 loss = 1994.46630859, mse = 21700196352.00000000\n",
      "[2025-02-09 02:00:27] 80 loss = 1994.46630859, mse = 21700196352.00000000\n",
      "[2025-02-09 02:00:27] 90 loss = 1994.46630859, mse = 21700196352.00000000\n",
      "[2025-02-09 02:00:28] 100 loss = 1994.46630859, mse = 21700196352.00000000\n",
      "[2025-02-09 02:00:28] 110 loss = 1994.46630859, mse = 21700196352.00000000\n",
      "[2025-02-09 02:00:29] 120 loss = 1994.46630859, mse = 21700196352.00000000\n",
      "[2025-02-09 02:00:29] 130 loss = 1994.46630859, mse = 21700196352.00000000\n",
      "[2025-02-09 02:00:30] 140 loss = 1994.46630859, mse = 21700196352.00000000\n",
      "[2025-02-09 02:00:30] 150 loss = 1994.46630859, mse = 21700196352.00000000\n",
      "[2025-02-09 02:00:31] 160 loss = 1994.46630859, mse = 21700196352.00000000\n",
      "[2025-02-09 02:00:32] 170 loss = 1994.46630859, mse = 21700196352.00000000\n",
      "[2025-02-09 02:00:32] 180 loss = 1994.46630859, mse = 21700196352.00000000\n",
      "[2025-02-09 02:00:33] 190 loss = 1994.46630859, mse = 21700196352.00000000\n",
      "[2025-02-09 02:00:34] 200 loss = 1994.46630859, mse = 21700196352.00000000\n",
      "[2025-02-09 02:00:35] 210 loss = 1994.46630859, mse = 21700196352.00000000\n",
      "[2025-02-09 02:00:36] 220 loss = 1994.46630859, mse = 21700196352.00000000\n",
      "[2025-02-09 02:00:37] 230 loss = 1994.46630859, mse = 21700196352.00000000\n",
      "[2025-02-09 02:00:37] 240 loss = 1994.46630859, mse = 21700196352.00000000\n",
      "[2025-02-09 02:00:38] 250 loss = 1994.46630859, mse = 21700196352.00000000\n",
      "[2025-02-09 02:00:39] 260 loss = 1994.46630859, mse = 21700196352.00000000\n",
      "[2025-02-09 02:00:40] 270 loss = 1994.46630859, mse = 21700196352.00000000\n",
      "[2025-02-09 02:00:42] 280 loss = 1994.46630859, mse = 21700196352.00000000\n",
      "[2025-02-09 02:00:43] 290 loss = 1994.46630859, mse = 21700196352.00000000\n",
      "imidx_list: [9757]\n",
      "loss_DLG: 1994.46630859375\n",
      "mse_DLG: 21700196352.0\n",
      "gt_label: [43] lab_DLG: 30\n",
      "----------------------\n",
      "\n",
      "\n",
      "running 9|10 experiment\n",
      "DLG, Try to generate 1 images\n",
      "lr = 1.0\n",
      "[2025-02-09 02:00:44] 0 loss = 34.40345764, mse = 1.09820497\n",
      "[2025-02-09 02:00:47] 10 loss = 0.32658535, mse = 0.19689527\n",
      "[2025-02-09 02:00:49] 20 loss = 0.02005023, mse = 0.04310849\n",
      "[2025-02-09 02:00:52] 30 loss = 0.00292830, mse = 0.01290412\n",
      "[2025-02-09 02:00:54] 40 loss = 0.00065250, mse = 0.00430090\n",
      "[2025-02-09 02:00:57] 50 loss = 0.00020715, mse = 0.00172584\n",
      "[2025-02-09 02:01:00] 60 loss = 0.00006768, mse = 0.00068975\n",
      "[2025-02-09 02:01:03] 70 loss = 0.00002896, mse = 0.00032517\n",
      "[2025-02-09 02:01:06] 80 loss = 0.00001442, mse = 0.00015701\n",
      "[2025-02-09 02:01:08] 90 loss = 0.00000907, mse = 0.00008804\n",
      "[2025-02-09 02:01:10] 100 loss = 0.00000614, mse = 0.00004800\n",
      "[2025-02-09 02:01:12] 110 loss = 0.00000467, mse = 0.00002931\n",
      "[2025-02-09 02:01:14] 120 loss = 0.00000381, mse = 0.00001903\n",
      "[2025-02-09 02:01:16] 130 loss = 0.00000338, mse = 0.00001449\n",
      "[2025-02-09 02:01:18] 140 loss = 0.00000305, mse = 0.00001120\n",
      "[2025-02-09 02:01:20] 150 loss = 0.00000275, mse = 0.00000880\n",
      "[2025-02-09 02:01:23] 160 loss = 0.00000253, mse = 0.00000701\n",
      "[2025-02-09 02:01:25] 170 loss = 0.00000242, mse = 0.00000638\n",
      "[2025-02-09 02:01:27] 180 loss = 0.00000225, mse = 0.00000516\n",
      "[2025-02-09 02:01:29] 190 loss = 0.00000217, mse = 0.00000460\n",
      "[2025-02-09 02:01:30] 200 loss = 0.00000210, mse = 0.00000418\n",
      "[2025-02-09 02:01:32] 210 loss = 0.00000202, mse = 0.00000363\n",
      "[2025-02-09 02:01:34] 220 loss = 0.00000194, mse = 0.00000318\n",
      "[2025-02-09 02:01:35] 230 loss = 0.00000192, mse = 0.00000302\n",
      "[2025-02-09 02:01:36] 240 loss = 0.00000192, mse = 0.00000302\n",
      "[2025-02-09 02:01:37] 250 loss = 0.00000192, mse = 0.00000302\n",
      "[2025-02-09 02:01:38] 260 loss = 0.00000192, mse = 0.00000302\n",
      "[2025-02-09 02:01:39] 270 loss = 0.00000192, mse = 0.00000302\n",
      "[2025-02-09 02:01:40] 280 loss = 0.00000192, mse = 0.00000302\n",
      "[2025-02-09 02:01:42] 290 loss = 0.00000192, mse = 0.00000302\n",
      "imidx_list: [316]\n",
      "loss_DLG: 1.923714535223553e-06\n",
      "mse_DLG: 3.02229682347388e-06\n",
      "gt_label: [42] lab_DLG: 42\n",
      "----------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = 'cifar100'\n",
    "root_path = '.'\n",
    "data_path = os.path.join(root_path, '../Data')\n",
    "save_path = os.path.join(root_path, 'results/DLG_%s'%dataset)\n",
    "\n",
    "lr = 1.0\n",
    "num_dummy = 1\n",
    "Iteration = 300\n",
    "num_exp = 10\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = 'cuda' if use_cuda else 'cpu'\n",
    "\n",
    "tt = transforms.Compose([transforms.ToTensor()])\n",
    "tp = transforms.Compose([transforms.ToPILImage()])\n",
    "\n",
    "print(dataset, 'root_path:', root_path)\n",
    "print(dataset, 'data_path:', data_path)\n",
    "print(dataset, 'save_path:', save_path)\n",
    "\n",
    "if not os.path.exists('results'):\n",
    "    os.mkdir('results')\n",
    "if not os.path.exists(save_path):\n",
    "    os.mkdir(save_path)\n",
    "\n",
    "\n",
    "\n",
    "''' load data '''\n",
    "\n",
    "if dataset == 'cifar100':\n",
    "    shape_img = (32, 32)\n",
    "    num_classes = 100\n",
    "    channel = 3\n",
    "    hidden = 768\n",
    "    dst = datasets.CIFAR100(data_path, download=True)\n",
    "\n",
    "\n",
    "\n",
    "''' train DLG and iDLG '''\n",
    "for idx_net in range(num_exp):\n",
    "    net = LeNet(channel=channel, hideen=hidden, num_classes=num_classes)\n",
    "    net.apply(weights_init)\n",
    "\n",
    "    print('running %d|%d experiment'%(idx_net, num_exp))########\n",
    "    net = net.to(device)\n",
    "    idx_shuffle = np.random.permutation(len(dst))\n",
    "\n",
    "    print('%s, Try to generate %d images' % (\"DLG\", num_dummy))\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    imidx_list = []\n",
    "\n",
    "    for imidx in range(num_dummy):\n",
    "        idx = idx_shuffle[imidx]\n",
    "        imidx_list.append(idx)\n",
    "        tmp_datum = tt(dst[idx][0]).float().to(device)\n",
    "        tmp_datum = tmp_datum.view(1, *tmp_datum.size())\n",
    "        tmp_label = torch.Tensor([dst[idx][1]]).long().to(device)\n",
    "        tmp_label = tmp_label.view(1, )\n",
    "        if imidx == 0:\n",
    "            gt_data = tmp_datum\n",
    "            gt_label = tmp_label\n",
    "        else:\n",
    "            gt_data = torch.cat((gt_data, tmp_datum), dim=0)\n",
    "            gt_label = torch.cat((gt_label, tmp_label), dim=0)\n",
    "\n",
    "\n",
    "    # compute original gradient\n",
    "    out = net(gt_data)\n",
    "    y = criterion(out, gt_label)\n",
    "    dy_dx = torch.autograd.grad(y, net.parameters())\n",
    "    original_dy_dx = list((_.detach().clone() for _ in dy_dx))\n",
    "\n",
    "    # generate dummy data and label\n",
    "    dummy_data = torch.randn(gt_data.size()).to(device).requires_grad_(True)\n",
    "    dummy_label = torch.randn((gt_data.shape[0], num_classes)).to(device).requires_grad_(True)\n",
    "\n",
    "    optimizer = torch.optim.LBFGS([dummy_data, dummy_label], lr=lr)\n",
    "\n",
    "\n",
    "    history = []\n",
    "    history_iters = []\n",
    "    losses = []\n",
    "    mses = []\n",
    "    train_iters = []\n",
    "\n",
    "    print('lr =', lr)\n",
    "    for iters in range(Iteration):\n",
    "\n",
    "        def closure():\n",
    "            optimizer.zero_grad()\n",
    "            pred = net(dummy_data)\n",
    "\n",
    "            dummy_loss = - torch.mean(torch.sum(torch.softmax(dummy_label, -1) * torch.log(torch.softmax(pred, -1)), dim=-1))\n",
    "            # dummy_loss = criterion(pred, gt_label)\n",
    "\n",
    "\n",
    "            dummy_dy_dx = torch.autograd.grad(dummy_loss, net.parameters(), create_graph=True)\n",
    "\n",
    "            grad_diff = 0\n",
    "            for gx, gy in zip(dummy_dy_dx, original_dy_dx):\n",
    "                grad_diff += ((gx - gy) ** 2).sum()\n",
    "            grad_diff.backward()\n",
    "            return grad_diff\n",
    "\n",
    "        optimizer.step(closure)\n",
    "        current_loss = closure().item()\n",
    "        train_iters.append(iters)\n",
    "        losses.append(current_loss)\n",
    "        mses.append(torch.mean((dummy_data-gt_data)**2).item())\n",
    "\n",
    "\n",
    "        if iters % int(Iteration / 30) == 0:\n",
    "            current_time = str(time.strftime(\"[%Y-%m-%d %H:%M:%S]\", time.localtime()))\n",
    "            print(current_time, iters, 'loss = %.8f, mse = %.8f' %(current_loss, mses[-1]))\n",
    "            history.append([tp(dummy_data[imidx].cpu()) for imidx in range(num_dummy)])\n",
    "            history_iters.append(iters)\n",
    "\n",
    "            for imidx in range(num_dummy):\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                plt.subplot(3, 10, 1)\n",
    "                plt.imshow(tp(gt_data[imidx].cpu()))\n",
    "                for i in range(min(len(history), 29)):\n",
    "                    plt.subplot(3, 10, i + 2)\n",
    "                    plt.imshow(history[i][imidx])\n",
    "                    plt.title('iter=%d' % (history_iters[i]))\n",
    "                    plt.axis('off')\n",
    "\n",
    "                    plt.savefig('%s/DLG_on_%s_%05d.png' % (save_path, imidx_list, imidx_list[imidx]))\n",
    "                    plt.close()\n",
    "\n",
    "\n",
    "            if current_loss < 0.000001: # converge\n",
    "                break\n",
    "\n",
    "    loss_DLG = losses\n",
    "    label_DLG = torch.argmax(dummy_label, dim=-1).detach().item()\n",
    "    mse_DLG = mses\n",
    "\n",
    "\n",
    "    print('imidx_list:', imidx_list)\n",
    "    print('loss_DLG:', loss_DLG[-1], )\n",
    "    print('mse_DLG:', mse_DLG[-1])\n",
    "    print('gt_label:', gt_label.detach().cpu().data.numpy(), 'lab_DLG:', label_DLG, )\n",
    "\n",
    "    print('----------------------\\n\\n')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
